services:
  fastapi_app:
    build:
      context: . # Keep the build context as the root directory
      dockerfile: dev.gpu.Dockerfile # Specify the new path to the GPU Dockerfile
    entrypoint: /app/scripts/entrypoint.sh
    ports:
      - "8000:8000"
    environment:
      - APP_TYPE=fastapi
      - CELERY_BROKER_URL=${CELERY_BROKER_URL-redis://redis:6379/0}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND-redis://redis:6379/0}
      - LLM_PULL_API_URL=${LLM_PULL_API_URL-http://web:8000/llm_pull}
      - LLM_GENEREATE_API_URL=${LLM_GENEREATE_API_URL-http://web:8000/llm_generate}
      - OLLAMA_HOST=${OLLAMA_HOST-http://ollama:11434}
      - APP_ENV=${APP_ENV-development}  # Default to development mode
      - STORAGE_PROFILE_PATH=${STORAGE_PROFILE_PATH-/app/storage_profiles}  # Add the storage profile path
      - LIST_FILES_URL=${LIST_FILES_URL-http://localhost:8000/storage/list}
      - LOAD_FILE_URL=${LOAD_FILE_URL-http://localhost:8000/storage/load}
      - DELETE_FILE_URL=${DELETE_FILE_URL-http://localhost:8000/storage/delete}
      - LLAMA_VISION_PROMPT=${LLAMA_VISION_PROMPT-"You are OCR. Convert image to markdown."}
    depends_on:
      - redis
      - ollama
    volumes: &appvolumes
      - .:/app  # Mount the app directory to enable auto-reloading
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]  # Request GPU support

  celery_worker:
    build:
      context: .  # Keep the build context as the root directory
      dockerfile: dev.gpu.Dockerfile # Specify the new path to the GPU Dockerfile
    entrypoint: /app/scripts/entrypoint.sh
    environment:
      - APP_TYPE=celery
      - OLLAMA_HOST=${OLLAMA_HOST-http://ollama:11434}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL-redis://redis:6379/0}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND-redis://redis:6379/0}
      - STORAGE_PROFILE_PATH=${STORAGE_PROFILE_PATH-/app/storage_profiles}  # Add the storage profile path
      - LIST_FILES_URL=${LIST_FILES_URL-http://localhost:8000/storage/list}
      - LOAD_FILE_URL=${LOAD_FILE_URL-http://localhost:8000/storage/load}
      - DELETE_FILE_URL=${DELETE_FILE_URL-http://localhost:8000/storage/delete}
      - LLAMA_VISION_PROMPT=${LLAMA_VISION_PROMPT-"You are OCR. Convert image to markdown."}
    depends_on:
      - redis
      - fastapi_app
    volumes: *appvolumes
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]  # Request GPU support

  redis:
    image: redis:7.2.4-alpine
    ports:
      - "${REDIS_HOST_PORT:-6379}:6379"

  ollama:
    image: ollama/ollama # Use the official Ollama image
    pull_policy: always
    tty: true
    restart: always
    ports:
      - "11434:11434"  # Expose Ollama's API port, changing internal to external port
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]  # Assumes health endpoint exists
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
